{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Amazon_Reviews_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UAYHVIE1f4d"
      },
      "source": [
        "# Imports\n",
        "import json\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import torch\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "!pip install transformers\n",
        "from transformers import BertTokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import time\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUPu2sCT11s-"
      },
      "source": [
        "# Read in data file\n",
        "\n",
        "# Opening JSON file\n",
        "reviews = open('Clothing_Shoes_and_Jewelry_5.json',) \n",
        "\n",
        "# returns JSON object as  \n",
        "# a dictionary \n",
        "data = json.loads(reviews.read())"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyTqlV_E114r",
        "outputId": "4cca6059-ca70-4188-f45f-801b637df6a8"
      },
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device('cuda')\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device('cpu')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PUgibs912NC"
      },
      "source": [
        "# Iterate through data points and grab reviews\n",
        "sentences = []; ratings = []\n",
        "for review in data[\"reviews\"]:\n",
        "    # Append to sentences\n",
        "    sentences.append(review['reviewText'])\n",
        "    ratings.append(review['overall'])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "Im8LTjAUTnzU",
        "outputId": "3bea37b4-6a4e-4021-88a3-abafe2e04342"
      },
      "source": [
        "# Plotting bar chart of labels\n",
        "ratingsToPlot = Counter(ratings)\n",
        "plt.bar(ratingsToPlot.keys(), ratingsToPlot.values(), align='center')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Count')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjUnmNciTV1U"
      },
      "source": [
        "# Formatting labels\n",
        "ratings = [rating - 1 for rating in ratings]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EW0NU-VT12WC",
        "outputId": "31037f0c-ad1f-4cf5-920e-8270c0ca4891"
      },
      "source": [
        "# Tokenizing\n",
        "# Load the BERT tokenizer.\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Showing an example of the tokenized sentences\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Original:  This is a great tutu and at a really great price. It doesn't look cheap at all. I'm so glad I looked on Amazon and found such an affordable tutu that isn't made poorly. A++\n",
            "Token IDs: [101, 2023, 2003, 1037, 2307, 10722, 8525, 1998, 2012, 1037, 2428, 2307, 3976, 1012, 2009, 2987, 1005, 1056, 2298, 10036, 2012, 2035, 1012, 1045, 1005, 1049, 2061, 5580, 1045, 2246, 2006, 9733, 1998, 2179, 2107, 2019, 15184, 10722, 8525, 2008, 3475, 1005, 1056, 2081, 9996, 1012, 1037, 1009, 1009, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBh-yd-H12ek",
        "outputId": "fbc02eb2-6d80-4039-a2d2-9954328999eb"
      },
      "source": [
        "# Padding\n",
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))\n",
        "\n",
        "MAX_LEN = 512 # MAX that BERT model can accept\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "print('\\Done.')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  5417\n",
            "\n",
            "Padding/truncating all sentences to 512 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\\Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HXYtB7s12lr"
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FhQ2BPYB0BG"
      },
      "source": [
        "# Data splitting\n",
        "\n",
        "# Splitting data into train and test (4:1)\n",
        "train_inputs, test_inputs, train_labels, test_labels = train_test_split(input_ids, ratings, \n",
        "                                                            random_state=1000, test_size=0.1)\n",
        "# Do the same for the masks.\n",
        "train_masks, test_masks, _, _ = train_test_split(attention_masks, ratings,\n",
        "                                                 random_state=1000, test_size=0.1)\n",
        "\n",
        "# Splitting training data into train set and validation set\n",
        "partial_train_inputs, partial_validation_inputs, partial_train_labels, partial_validation_labels = train_test_split(train_inputs, train_labels, \n",
        "                                                            random_state=2000, test_size=0.1)\n",
        "# Do the same for the masks.\n",
        "partial_train_masks, partial_validation_masks, _, _ = train_test_split(train_masks, train_labels,\n",
        "                                             random_state=2000, test_size=0.1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGhvh_nKB0Q7"
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "partial_train_inputs = torch.tensor(partial_train_inputs)\n",
        "partial_validation_inputs = torch.tensor(partial_validation_inputs)\n",
        "test_inputs = torch.tensor(test_inputs)\n",
        "\n",
        "partial_train_labels = torch.tensor(partial_train_labels).type(torch.LongTensor)\n",
        "partial_validation_labels = torch.tensor(partial_validation_labels).type(torch.LongTensor)\n",
        "test_labels = torch.tensor(test_labels).type(torch.LongTensor)\n",
        "\n",
        "partial_train_masks = torch.tensor(partial_train_masks)\n",
        "partial_validation_masks = torch.tensor(partial_validation_masks)\n",
        "test_masks = torch.tensor(test_masks)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuBRurpSB0XS"
      },
      "source": [
        "# Doing the data loaders for the three sets\n",
        "batch_size = 15\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(partial_train_inputs, partial_train_masks, partial_train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(partial_validation_inputs, partial_validation_masks, partial_validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our testing set.\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPiH8Y0CB0eC",
        "outputId": "5143d23c-f27f-478b-c950-677f63e4eb59"
      },
      "source": [
        "# BERT Model\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 5, # 5 classes  \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZR8qGmJ12tp"
      },
      "source": [
        "# Adam Optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5,\n",
        "                  eps = 1e-8\n",
        "                )\n",
        "\n",
        "# Number of training epochs\n",
        "epochs = 2\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBNtn9jJCJ3J"
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nrh2J49YCJ-N"
      },
      "source": [
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMvPSN9pCKGO",
        "outputId": "b43c45f4-cf22-451e-fd20-7b59b4d893b5"
      },
      "source": [
        "# TRAINING\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "    # Put the model into training mode. \n",
        "    model.train()\n",
        "    # For each batch of training data...\n",
        "    eval_train_accuracy = 0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "        # Unpack this training batch from our dataloader\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Clearing any previously calculated gradients before performing a\n",
        "        # backward pass.\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Evaluate the model on this training batch\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "        \n",
        "        # Get the \"logits\" output by the model.\n",
        "        logits = outputs[1]\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_train_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_train_accuracy += tmp_train_eval_accuracy\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_acc = eval_train_accuracy / len(train_dataloader)  \n",
        "\n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Average training accuracy: {0:.2f}\".format(avg_train_acc))\n",
        "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "    t0 = time.time()\n",
        "    # Put the model in evaluation mode\n",
        "    model.eval()\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Not computing gradients during validation\n",
        "        with torch.no_grad():\n",
        "            # Evaluate the model on this validation batch\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model\n",
        "        logits = outputs[0]\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of  5,400.    Elapsed: 0:00:52.\n",
            "  Batch    80  of  5,400.    Elapsed: 0:01:46.\n",
            "  Batch   120  of  5,400.    Elapsed: 0:02:45.\n",
            "  Batch   160  of  5,400.    Elapsed: 0:03:45.\n",
            "  Batch   200  of  5,400.    Elapsed: 0:04:45.\n",
            "  Batch   240  of  5,400.    Elapsed: 0:05:45.\n",
            "  Batch   280  of  5,400.    Elapsed: 0:06:45.\n",
            "  Batch   320  of  5,400.    Elapsed: 0:07:45.\n",
            "  Batch   360  of  5,400.    Elapsed: 0:08:45.\n",
            "  Batch   400  of  5,400.    Elapsed: 0:09:45.\n",
            "  Batch   440  of  5,400.    Elapsed: 0:10:45.\n",
            "  Batch   480  of  5,400.    Elapsed: 0:11:45.\n",
            "  Batch   520  of  5,400.    Elapsed: 0:12:45.\n",
            "  Batch   560  of  5,400.    Elapsed: 0:13:45.\n",
            "  Batch   600  of  5,400.    Elapsed: 0:14:45.\n",
            "  Batch   640  of  5,400.    Elapsed: 0:15:45.\n",
            "  Batch   680  of  5,400.    Elapsed: 0:16:45.\n",
            "  Batch   720  of  5,400.    Elapsed: 0:17:45.\n",
            "  Batch   760  of  5,400.    Elapsed: 0:18:45.\n",
            "  Batch   800  of  5,400.    Elapsed: 0:19:44.\n",
            "  Batch   840  of  5,400.    Elapsed: 0:20:44.\n",
            "  Batch   880  of  5,400.    Elapsed: 0:21:44.\n",
            "  Batch   920  of  5,400.    Elapsed: 0:22:44.\n",
            "  Batch   960  of  5,400.    Elapsed: 0:23:44.\n",
            "  Batch 1,000  of  5,400.    Elapsed: 0:24:44.\n",
            "  Batch 1,040  of  5,400.    Elapsed: 0:25:44.\n",
            "  Batch 1,080  of  5,400.    Elapsed: 0:26:44.\n",
            "  Batch 1,120  of  5,400.    Elapsed: 0:27:44.\n",
            "  Batch 1,160  of  5,400.    Elapsed: 0:28:44.\n",
            "  Batch 1,200  of  5,400.    Elapsed: 0:29:44.\n",
            "  Batch 1,240  of  5,400.    Elapsed: 0:30:44.\n",
            "  Batch 1,280  of  5,400.    Elapsed: 0:31:43.\n",
            "  Batch 1,320  of  5,400.    Elapsed: 0:32:43.\n",
            "  Batch 1,360  of  5,400.    Elapsed: 0:33:43.\n",
            "  Batch 1,400  of  5,400.    Elapsed: 0:34:43.\n",
            "  Batch 1,440  of  5,400.    Elapsed: 0:35:43.\n",
            "  Batch 1,480  of  5,400.    Elapsed: 0:36:43.\n",
            "  Batch 1,520  of  5,400.    Elapsed: 0:37:42.\n",
            "  Batch 1,560  of  5,400.    Elapsed: 0:38:42.\n",
            "  Batch 1,600  of  5,400.    Elapsed: 0:39:42.\n",
            "  Batch 1,640  of  5,400.    Elapsed: 0:40:42.\n",
            "  Batch 1,680  of  5,400.    Elapsed: 0:41:42.\n",
            "  Batch 1,720  of  5,400.    Elapsed: 0:42:42.\n",
            "  Batch 1,760  of  5,400.    Elapsed: 0:43:42.\n",
            "  Batch 1,800  of  5,400.    Elapsed: 0:44:42.\n",
            "  Batch 1,840  of  5,400.    Elapsed: 0:45:42.\n",
            "  Batch 1,880  of  5,400.    Elapsed: 0:46:41.\n",
            "  Batch 1,920  of  5,400.    Elapsed: 0:47:41.\n",
            "  Batch 1,960  of  5,400.    Elapsed: 0:48:41.\n",
            "  Batch 2,000  of  5,400.    Elapsed: 0:49:41.\n",
            "  Batch 2,040  of  5,400.    Elapsed: 0:50:41.\n",
            "  Batch 2,080  of  5,400.    Elapsed: 0:51:41.\n",
            "  Batch 2,120  of  5,400.    Elapsed: 0:52:41.\n",
            "  Batch 2,160  of  5,400.    Elapsed: 0:53:41.\n",
            "  Batch 2,200  of  5,400.    Elapsed: 0:54:41.\n",
            "  Batch 2,240  of  5,400.    Elapsed: 0:55:41.\n",
            "  Batch 2,280  of  5,400.    Elapsed: 0:56:41.\n",
            "  Batch 2,320  of  5,400.    Elapsed: 0:57:40.\n",
            "  Batch 2,360  of  5,400.    Elapsed: 0:58:40.\n",
            "  Batch 2,400  of  5,400.    Elapsed: 0:59:40.\n",
            "  Batch 2,440  of  5,400.    Elapsed: 1:00:40.\n",
            "  Batch 2,480  of  5,400.    Elapsed: 1:01:40.\n",
            "  Batch 2,520  of  5,400.    Elapsed: 1:02:40.\n",
            "  Batch 2,560  of  5,400.    Elapsed: 1:03:40.\n",
            "  Batch 2,600  of  5,400.    Elapsed: 1:04:40.\n",
            "  Batch 2,640  of  5,400.    Elapsed: 1:05:40.\n",
            "  Batch 2,680  of  5,400.    Elapsed: 1:06:39.\n",
            "  Batch 2,720  of  5,400.    Elapsed: 1:07:39.\n",
            "  Batch 2,760  of  5,400.    Elapsed: 1:08:39.\n",
            "  Batch 2,800  of  5,400.    Elapsed: 1:09:39.\n",
            "  Batch 2,840  of  5,400.    Elapsed: 1:10:39.\n",
            "  Batch 2,880  of  5,400.    Elapsed: 1:11:39.\n",
            "  Batch 2,920  of  5,400.    Elapsed: 1:12:39.\n",
            "  Batch 2,960  of  5,400.    Elapsed: 1:13:39.\n",
            "  Batch 3,000  of  5,400.    Elapsed: 1:14:39.\n",
            "  Batch 3,040  of  5,400.    Elapsed: 1:15:39.\n",
            "  Batch 3,080  of  5,400.    Elapsed: 1:16:38.\n",
            "  Batch 3,120  of  5,400.    Elapsed: 1:17:38.\n",
            "  Batch 3,160  of  5,400.    Elapsed: 1:18:38.\n",
            "  Batch 3,200  of  5,400.    Elapsed: 1:19:38.\n",
            "  Batch 3,240  of  5,400.    Elapsed: 1:20:38.\n",
            "  Batch 3,280  of  5,400.    Elapsed: 1:21:38.\n",
            "  Batch 3,320  of  5,400.    Elapsed: 1:22:38.\n",
            "  Batch 3,360  of  5,400.    Elapsed: 1:23:38.\n",
            "  Batch 3,400  of  5,400.    Elapsed: 1:24:38.\n",
            "  Batch 3,440  of  5,400.    Elapsed: 1:25:37.\n",
            "  Batch 3,480  of  5,400.    Elapsed: 1:26:37.\n",
            "  Batch 3,520  of  5,400.    Elapsed: 1:27:37.\n",
            "  Batch 3,560  of  5,400.    Elapsed: 1:28:37.\n",
            "  Batch 3,600  of  5,400.    Elapsed: 1:29:37.\n",
            "  Batch 3,640  of  5,400.    Elapsed: 1:30:37.\n",
            "  Batch 3,680  of  5,400.    Elapsed: 1:31:36.\n",
            "  Batch 3,720  of  5,400.    Elapsed: 1:32:36.\n",
            "  Batch 3,760  of  5,400.    Elapsed: 1:33:36.\n",
            "  Batch 3,800  of  5,400.    Elapsed: 1:34:36.\n",
            "  Batch 3,840  of  5,400.    Elapsed: 1:35:36.\n",
            "  Batch 3,880  of  5,400.    Elapsed: 1:36:35.\n",
            "  Batch 3,920  of  5,400.    Elapsed: 1:37:35.\n",
            "  Batch 3,960  of  5,400.    Elapsed: 1:38:35.\n",
            "  Batch 4,000  of  5,400.    Elapsed: 1:39:35.\n",
            "  Batch 4,040  of  5,400.    Elapsed: 1:40:35.\n",
            "  Batch 4,080  of  5,400.    Elapsed: 1:41:35.\n",
            "  Batch 4,120  of  5,400.    Elapsed: 1:42:35.\n",
            "  Batch 4,160  of  5,400.    Elapsed: 1:43:35.\n",
            "  Batch 4,200  of  5,400.    Elapsed: 1:44:34.\n",
            "  Batch 4,240  of  5,400.    Elapsed: 1:45:34.\n",
            "  Batch 4,280  of  5,400.    Elapsed: 1:46:34.\n",
            "  Batch 4,320  of  5,400.    Elapsed: 1:47:34.\n",
            "  Batch 4,360  of  5,400.    Elapsed: 1:48:34.\n",
            "  Batch 4,400  of  5,400.    Elapsed: 1:49:34.\n",
            "  Batch 4,440  of  5,400.    Elapsed: 1:50:34.\n",
            "  Batch 4,480  of  5,400.    Elapsed: 1:51:34.\n",
            "  Batch 4,520  of  5,400.    Elapsed: 1:52:34.\n",
            "  Batch 4,560  of  5,400.    Elapsed: 1:53:34.\n",
            "  Batch 4,600  of  5,400.    Elapsed: 1:54:33.\n",
            "  Batch 4,640  of  5,400.    Elapsed: 1:55:33.\n",
            "  Batch 4,680  of  5,400.    Elapsed: 1:56:33.\n",
            "  Batch 4,720  of  5,400.    Elapsed: 1:57:33.\n",
            "  Batch 4,760  of  5,400.    Elapsed: 1:58:33.\n",
            "  Batch 4,800  of  5,400.    Elapsed: 1:59:33.\n",
            "  Batch 4,840  of  5,400.    Elapsed: 2:00:33.\n",
            "  Batch 4,880  of  5,400.    Elapsed: 2:01:33.\n",
            "  Batch 4,920  of  5,400.    Elapsed: 2:02:33.\n",
            "  Batch 4,960  of  5,400.    Elapsed: 2:03:32.\n",
            "  Batch 5,000  of  5,400.    Elapsed: 2:04:32.\n",
            "  Batch 5,040  of  5,400.    Elapsed: 2:05:32.\n",
            "  Batch 5,080  of  5,400.    Elapsed: 2:06:32.\n",
            "  Batch 5,120  of  5,400.    Elapsed: 2:07:32.\n",
            "  Batch 5,160  of  5,400.    Elapsed: 2:08:32.\n",
            "  Batch 5,200  of  5,400.    Elapsed: 2:09:32.\n",
            "  Batch 5,240  of  5,400.    Elapsed: 2:10:32.\n",
            "  Batch 5,280  of  5,400.    Elapsed: 2:11:32.\n",
            "  Batch 5,320  of  5,400.    Elapsed: 2:12:32.\n",
            "  Batch 5,360  of  5,400.    Elapsed: 2:13:32.\n",
            "\n",
            "  Average training loss: 0.69\n",
            "  Average training accuracy: 0.72\n",
            "  Training epoch took: 2:14:32\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.74\n",
            "  Validation took: 0:05:38\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of  5,400.    Elapsed: 0:01:00.\n",
            "  Batch    80  of  5,400.    Elapsed: 0:02:00.\n",
            "  Batch   120  of  5,400.    Elapsed: 0:03:00.\n",
            "  Batch   160  of  5,400.    Elapsed: 0:04:00.\n",
            "  Batch   200  of  5,400.    Elapsed: 0:05:00.\n",
            "  Batch   240  of  5,400.    Elapsed: 0:06:00.\n",
            "  Batch   280  of  5,400.    Elapsed: 0:06:59.\n",
            "  Batch   320  of  5,400.    Elapsed: 0:08:00.\n",
            "  Batch   360  of  5,400.    Elapsed: 0:08:59.\n",
            "  Batch   400  of  5,400.    Elapsed: 0:10:00.\n",
            "  Batch   440  of  5,400.    Elapsed: 0:11:00.\n",
            "  Batch   480  of  5,400.    Elapsed: 0:11:59.\n",
            "  Batch   520  of  5,400.    Elapsed: 0:12:59.\n",
            "  Batch   560  of  5,400.    Elapsed: 0:13:59.\n",
            "  Batch   600  of  5,400.    Elapsed: 0:14:59.\n",
            "  Batch   640  of  5,400.    Elapsed: 0:15:59.\n",
            "  Batch   680  of  5,400.    Elapsed: 0:16:59.\n",
            "  Batch   720  of  5,400.    Elapsed: 0:17:59.\n",
            "  Batch   760  of  5,400.    Elapsed: 0:18:59.\n",
            "  Batch   800  of  5,400.    Elapsed: 0:19:59.\n",
            "  Batch   840  of  5,400.    Elapsed: 0:20:59.\n",
            "  Batch   880  of  5,400.    Elapsed: 0:21:59.\n",
            "  Batch   920  of  5,400.    Elapsed: 0:22:59.\n",
            "  Batch   960  of  5,400.    Elapsed: 0:23:59.\n",
            "  Batch 1,000  of  5,400.    Elapsed: 0:24:59.\n",
            "  Batch 1,040  of  5,400.    Elapsed: 0:25:59.\n",
            "  Batch 1,080  of  5,400.    Elapsed: 0:26:59.\n",
            "  Batch 1,120  of  5,400.    Elapsed: 0:27:59.\n",
            "  Batch 1,160  of  5,400.    Elapsed: 0:28:59.\n",
            "  Batch 1,200  of  5,400.    Elapsed: 0:29:58.\n",
            "  Batch 1,240  of  5,400.    Elapsed: 0:30:58.\n",
            "  Batch 1,280  of  5,400.    Elapsed: 0:31:58.\n",
            "  Batch 1,320  of  5,400.    Elapsed: 0:32:58.\n",
            "  Batch 1,360  of  5,400.    Elapsed: 0:33:58.\n",
            "  Batch 1,400  of  5,400.    Elapsed: 0:34:58.\n",
            "  Batch 1,440  of  5,400.    Elapsed: 0:35:58.\n",
            "  Batch 1,480  of  5,400.    Elapsed: 0:36:58.\n",
            "  Batch 1,520  of  5,400.    Elapsed: 0:37:58.\n",
            "  Batch 1,560  of  5,400.    Elapsed: 0:38:58.\n",
            "  Batch 1,600  of  5,400.    Elapsed: 0:39:57.\n",
            "  Batch 1,640  of  5,400.    Elapsed: 0:40:57.\n",
            "  Batch 1,680  of  5,400.    Elapsed: 0:41:57.\n",
            "  Batch 1,720  of  5,400.    Elapsed: 0:42:57.\n",
            "  Batch 1,760  of  5,400.    Elapsed: 0:43:57.\n",
            "  Batch 1,800  of  5,400.    Elapsed: 0:44:57.\n",
            "  Batch 1,840  of  5,400.    Elapsed: 0:45:57.\n",
            "  Batch 1,880  of  5,400.    Elapsed: 0:46:57.\n",
            "  Batch 1,920  of  5,400.    Elapsed: 0:47:57.\n",
            "  Batch 1,960  of  5,400.    Elapsed: 0:48:56.\n",
            "  Batch 2,000  of  5,400.    Elapsed: 0:49:56.\n",
            "  Batch 2,040  of  5,400.    Elapsed: 0:50:56.\n",
            "  Batch 2,080  of  5,400.    Elapsed: 0:51:57.\n",
            "  Batch 2,120  of  5,400.    Elapsed: 0:52:57.\n",
            "  Batch 2,160  of  5,400.    Elapsed: 0:53:56.\n",
            "  Batch 2,200  of  5,400.    Elapsed: 0:54:56.\n",
            "  Batch 2,240  of  5,400.    Elapsed: 0:55:57.\n",
            "  Batch 2,280  of  5,400.    Elapsed: 0:56:56.\n",
            "  Batch 2,320  of  5,400.    Elapsed: 0:57:57.\n",
            "  Batch 2,360  of  5,400.    Elapsed: 0:58:57.\n",
            "  Batch 2,400  of  5,400.    Elapsed: 0:59:56.\n",
            "  Batch 2,440  of  5,400.    Elapsed: 1:00:56.\n",
            "  Batch 2,480  of  5,400.    Elapsed: 1:01:56.\n",
            "  Batch 2,520  of  5,400.    Elapsed: 1:02:56.\n",
            "  Batch 2,560  of  5,400.    Elapsed: 1:03:56.\n",
            "  Batch 2,600  of  5,400.    Elapsed: 1:04:56.\n",
            "  Batch 2,640  of  5,400.    Elapsed: 1:05:56.\n",
            "  Batch 2,680  of  5,400.    Elapsed: 1:06:56.\n",
            "  Batch 2,720  of  5,400.    Elapsed: 1:07:56.\n",
            "  Batch 2,760  of  5,400.    Elapsed: 1:08:56.\n",
            "  Batch 2,800  of  5,400.    Elapsed: 1:09:56.\n",
            "  Batch 2,840  of  5,400.    Elapsed: 1:10:55.\n",
            "  Batch 2,880  of  5,400.    Elapsed: 1:11:55.\n",
            "  Batch 2,920  of  5,400.    Elapsed: 1:12:55.\n",
            "  Batch 2,960  of  5,400.    Elapsed: 1:13:55.\n",
            "  Batch 3,000  of  5,400.    Elapsed: 1:14:55.\n",
            "  Batch 3,040  of  5,400.    Elapsed: 1:15:55.\n",
            "  Batch 3,080  of  5,400.    Elapsed: 1:16:55.\n",
            "  Batch 3,120  of  5,400.    Elapsed: 1:17:55.\n",
            "  Batch 3,160  of  5,400.    Elapsed: 1:18:55.\n",
            "  Batch 3,200  of  5,400.    Elapsed: 1:19:55.\n",
            "  Batch 3,240  of  5,400.    Elapsed: 1:20:54.\n",
            "  Batch 3,280  of  5,400.    Elapsed: 1:21:55.\n",
            "  Batch 3,320  of  5,400.    Elapsed: 1:22:55.\n",
            "  Batch 3,360  of  5,400.    Elapsed: 1:23:54.\n",
            "  Batch 3,400  of  5,400.    Elapsed: 1:24:54.\n",
            "  Batch 3,440  of  5,400.    Elapsed: 1:25:54.\n",
            "  Batch 3,480  of  5,400.    Elapsed: 1:26:54.\n",
            "  Batch 3,520  of  5,400.    Elapsed: 1:27:53.\n",
            "  Batch 3,560  of  5,400.    Elapsed: 1:28:53.\n",
            "  Batch 3,600  of  5,400.    Elapsed: 1:29:53.\n",
            "  Batch 3,640  of  5,400.    Elapsed: 1:30:53.\n",
            "  Batch 3,680  of  5,400.    Elapsed: 1:31:53.\n",
            "  Batch 3,720  of  5,400.    Elapsed: 1:32:53.\n",
            "  Batch 3,760  of  5,400.    Elapsed: 1:33:53.\n",
            "  Batch 3,800  of  5,400.    Elapsed: 1:34:53.\n",
            "  Batch 3,840  of  5,400.    Elapsed: 1:35:53.\n",
            "  Batch 3,880  of  5,400.    Elapsed: 1:36:52.\n",
            "  Batch 3,920  of  5,400.    Elapsed: 1:37:52.\n",
            "  Batch 3,960  of  5,400.    Elapsed: 1:38:52.\n",
            "  Batch 4,000  of  5,400.    Elapsed: 1:39:52.\n",
            "  Batch 4,040  of  5,400.    Elapsed: 1:40:52.\n",
            "  Batch 4,080  of  5,400.    Elapsed: 1:41:52.\n",
            "  Batch 4,120  of  5,400.    Elapsed: 1:42:52.\n",
            "  Batch 4,160  of  5,400.    Elapsed: 1:43:52.\n",
            "  Batch 4,200  of  5,400.    Elapsed: 1:44:52.\n",
            "  Batch 4,240  of  5,400.    Elapsed: 1:45:51.\n",
            "  Batch 4,280  of  5,400.    Elapsed: 1:46:51.\n",
            "  Batch 4,320  of  5,400.    Elapsed: 1:47:51.\n",
            "  Batch 4,360  of  5,400.    Elapsed: 1:48:51.\n",
            "  Batch 4,400  of  5,400.    Elapsed: 1:49:51.\n",
            "  Batch 4,440  of  5,400.    Elapsed: 1:50:51.\n",
            "  Batch 4,480  of  5,400.    Elapsed: 1:51:51.\n",
            "  Batch 4,520  of  5,400.    Elapsed: 1:52:51.\n",
            "  Batch 4,560  of  5,400.    Elapsed: 1:53:50.\n",
            "  Batch 4,600  of  5,400.    Elapsed: 1:54:50.\n",
            "  Batch 4,640  of  5,400.    Elapsed: 1:55:50.\n",
            "  Batch 4,680  of  5,400.    Elapsed: 1:56:50.\n",
            "  Batch 4,720  of  5,400.    Elapsed: 1:57:50.\n",
            "  Batch 4,760  of  5,400.    Elapsed: 1:58:50.\n",
            "  Batch 4,800  of  5,400.    Elapsed: 1:59:50.\n",
            "  Batch 4,840  of  5,400.    Elapsed: 2:00:50.\n",
            "  Batch 4,880  of  5,400.    Elapsed: 2:01:50.\n",
            "  Batch 4,920  of  5,400.    Elapsed: 2:02:50.\n",
            "  Batch 4,960  of  5,400.    Elapsed: 2:03:49.\n",
            "  Batch 5,000  of  5,400.    Elapsed: 2:04:49.\n",
            "  Batch 5,040  of  5,400.    Elapsed: 2:05:49.\n",
            "  Batch 5,080  of  5,400.    Elapsed: 2:06:49.\n",
            "  Batch 5,120  of  5,400.    Elapsed: 2:07:49.\n",
            "  Batch 5,160  of  5,400.    Elapsed: 2:08:49.\n",
            "  Batch 5,200  of  5,400.    Elapsed: 2:09:49.\n",
            "  Batch 5,240  of  5,400.    Elapsed: 2:10:49.\n",
            "  Batch 5,280  of  5,400.    Elapsed: 2:11:49.\n",
            "  Batch 5,320  of  5,400.    Elapsed: 2:12:49.\n",
            "  Batch 5,360  of  5,400.    Elapsed: 2:13:48.\n",
            "\n",
            "  Average training loss: 0.55\n",
            "  Average training accuracy: 0.78\n",
            "  Training epoch took: 2:14:48\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.75\n",
            "  Validation took: 0:05:38\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaxfGBCyZCUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28f56e11-c074-4b2d-e328-7ef518e3a2eb"
      },
      "source": [
        "# Test Set\n",
        "print(\"Running Testing...\")\n",
        "t0 = time.time()\n",
        "# Put the model in evaluation mode--the dropout layers behave differently\n",
        "# during evaluation.\n",
        "model.eval()\n",
        "# Tracking variables \n",
        "eval_loss, eval_accuracy = 0, 0\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "for batch in test_dataloader:\n",
        "        \n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "  # Not computing gradients during testing\n",
        "  with torch.no_grad():\n",
        "    # Evaluate the model on this testing batch\n",
        "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "        \n",
        "    # Get the \"logits\" output by the model\n",
        "    logits = outputs[0]\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Calculate the accuracy for this batch of test sentences.\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "    # Accumulate the total accuracy.\n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    # Track the number of batches\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "# Report the final accuracy for testing.\n",
        "print(\"  Testing Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "print(\"  Testing took: {:}\".format(format_time(time.time() - t0)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running Testing...\n",
            "  Testing Accuracy: 0.75\n",
            "  Testing took: 0:06:15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvrdQs--6SiZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}